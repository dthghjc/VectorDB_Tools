---
alwaysApply: true
---
# VectorDB_Tools 项目设计文档

## 项目愿景 (Consolidated Vision)

本项目的目标是构建一个围绕数据处理工作流的任务管理平台，将从原始数据到 Milvus 可用数据的繁琐过程产品化、可视化。项目包含五大核心模块，构成完整的开发蓝图。

### 1. 配置中心 (Configuration)

- 管理 Milvus 连接配置。
- 管理并安全存储第三方服务（如向量模型 API）的密钥。

### 2. Schema 管理器 (Schema Management)

- 提供 UI 界面，让用户能够创建、编辑和保存 Milvus Collection 的 Schema 模板。
- 这些模板将在数据导入时被复用。

### 3. 数据导入流水线 (Data Ingestion Pipeline)

这是一个引导式的、分步骤的核心流程。
1.  **上传**: 用户上传原始数据文件。
2.  **向量化**: 用户选择向量模型，配置字段映射，系统将此作为一个后台任务来执行向量生成。
3.  **加载**: 用户选择向量化后的数据和目标 Schema, 系统将此作为另一个后台任务来执行 Milvus 数据导入。

### 4. 任务中心 (Task Management)

- 一个仪表盘 (Dashboard)，集中展示所有后台任务（向量化、数据加载、批量检索）的列表。
- 用户可以清晰地看到每个任务的状态（排队、运行中、成功、失败）、进度和日志。

### 5. 检索与评估中心 (Search & Evaluation)

- **目标**: 验证和测试向量数据的检索效果。
- **功能**:
    - 允许用户创建“测试集”（通过界面编辑或上传 prompt 文件）。
    - 用户选择一个 Collection，选择一个测试集，设置检索参数（如 `top_k`）。
    - 系统以后台任务的形式，批量执行测试集中的所有检索请求。
    - 提供一个结果页面，展示每一条 prompt 的检索结果（返回的实体、距离等）。
    - 支持将该次测试的所有结果导出为文件（如 CSV 或 JSON），便于离线分析和报告。

---

## 技术架构方案

### 一、 核心原则

- **前后端分离**: 严格遵守前后端分离原则，通过 RESTful API 进行通信。
- **异步任务处理**: 核心的、耗时的操作（数据向量化、加载）必须通过后台任务队列异步执行，前端通过 WebSocket 获取状态。

### 二、 后端架构 (Python + FastAPI)

后端是整个系统的核心，将采用分层架构并引入任务队列。

- **Web 框架**: FastAPI
- **数据库 ORM**: SQLAlchemy (配合 Alembic 进行数据库迁移)
- **数据库**: PostgreSQL
- **后台任务队列**: Celery + Redis
    - **Celery**: 业界标准的 Python 分布式任务队列，负责执行异步任务。
    - **Redis**: 作为 Celery 的 Broker (消息中间件) 和 Result Backend (结果存储)。它轻量、快速，非常适合这个场景。
- **数据验证**: Pydantic

### 三、 前端架构 (Vite + React)

前端负责提供流畅的用户交互体验。

- **UI 框架**: React (使用 Vite 构建)
- **样式**: TailwindCSS + shadcn/ui
- **状态管理**: Redux Toolkit (RTK)
    - 用于管理全局状态，如 Milvus 连接列表。
    - 特别适用于管理任务状态：当用户提交一个任务后，可以将任务列表存入 store，并定期（轮询）更新它们的状态。
- **路由**: React Router# Milvus-Tools 项目设计文档

## 项目愿景 (Consolidated Vision)

本项目的目标是构建一个围绕数据处理工作流的任务管理平台，将从原始数据到 Milvus 可用数据的繁琐过程产品化、可视化。项目包含五大核心模块，构成完整的开发蓝图。

### 1. 配置中心 (Configuration)

- 管理 Milvus 连接配置。
- 管理并安全存储第三方服务（如向量模型 API）的密钥。

### 2. Schema 管理器 (Schema Management)

- 提供 UI 界面，让用户能够创建、编辑和保存 Milvus Collection 的 Schema 模板。
- 这些模板将在数据导入时被复用。

### 3. 数据导入流水线 (Data Ingestion Pipeline)

这是一个引导式的、分步骤的核心流程。
1.  **上传**: 用户上传原始数据文件。
2.  **向量化**: 用户选择向量模型，配置字段映射，系统将此作为一个后台任务来执行向量生成。
3.  **加载**: 用户选择向量化后的数据和目标 Schema, 系统将此作为另一个后台任务来执行 Milvus 数据导入。

### 4. 任务中心 (Task Management)

- 一个仪表盘 (Dashboard)，集中展示所有后台任务（向量化、数据加载、批量检索）的列表。
- 用户可以清晰地看到每个任务的状态（排队、运行中、成功、失败）、进度和日志。

### 5. 检索与评估中心 (Search & Evaluation)

- **目标**: 验证和测试向量数据的检索效果。
- **功能**:
    - 允许用户创建“测试集”（通过界面编辑或上传 prompt 文件）。
    - 用户选择一个 Collection，选择一个测试集，设置检索参数（如 `top_k`）。
    - 系统以后台任务的形式，批量执行测试集中的所有检索请求。
    - 提供一个结果页面，展示每一条 prompt 的检索结果（返回的实体、距离等）。
    - 支持将该次测试的所有结果导出为文件（如 CSV 或 JSON），便于离线分析和报告。

---

## 技术架构方案

### 一、 核心原则

- **前后端分离**: 严格遵守前后端分离原则，通过 RESTful API 进行通信。
- **异步任务处理**: 核心的、耗时的操作（数据向量化、加载）必须通过后台任务队列异步执行，前端通过 WebSocket 获取状态。

### 二、 后端架构 (Python + FastAPI)

后端是整个系统的核心，将采用分层架构并引入任务队列。

- **Web 框架**: FastAPI
- **数据库 ORM**: SQLAlchemy (配合 Alembic 进行数据库迁移)
- **数据库**: PostgreSQL
- **后台任务队列**: Celery + Redis
    - **Celery**: 业界标准的 Python 分布式任务队列，负责执行异步任务。
    - **Redis**: 作为 Celery 的 Broker (消息中间件) 和 Result Backend (结果存储)。它轻量、快速，非常适合这个场景。
- **数据验证**: Pydantic

### 三、 前端架构 (Vite + React)

前端负责提供流畅的用户交互体验。

- **UI 框架**: React (使用 Vite 构建)
- **样式**: TailwindCSS + shadcn/ui
- **状态管理**: Redux Toolkit (RTK)
    - 用于管理全局状态，如 Milvus 连接列表。
    - 特别适用于管理任务状态：当用户提交一个任务后，可以将任务列表存入 store，并定期（轮询）更新它们的状态。
- **路由**: React Router